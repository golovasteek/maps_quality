{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "#import matplotlib.pyplot as plt\n",
    "from copy import deepcopy, copy\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "#%matplotlib inline\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_data():\n",
    "    empty = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "\n",
    "    images = []\n",
    "    for d, dirs, files in tqdm_notebook(list(os.walk('files/grayschema/18/'))):\n",
    "        for f in files:\n",
    "            total += 1\n",
    "            schema_path = os.path.join(d,f)\n",
    "            try:\n",
    "                if not os.path.isfile(schema_path):\n",
    "                    raise RuntimeError(\"{} doesn't exist\".format(schema_path))\n",
    "                img = cv2.imread(schema_path)\n",
    "                if (img <= 128).all():\n",
    "                    empty += 1\n",
    "                else:\n",
    "                    relpath = schema_path[len('files/grayschema/'):]\n",
    "                    sat_path = os.path.join('files/mapbox.satellite/', relpath)\n",
    "                    if not os.path.isfile(sat_path):\n",
    "                        raise RuntimeError(\"{} doesn't exist\".format(sat_path))\n",
    "                    cv2.imread(sat_path)\n",
    "                    images.append(relpath)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                error += 1\n",
    "    \n",
    "    print(\"Total {}, empty {} {:.2%}, error {}\".format(\n",
    "        total,\n",
    "        empty, empty * 1.0/ total,\n",
    "        error))\n",
    "    \n",
    "    data = []\n",
    "    shuffled_images = copy(images)\n",
    "    random.shuffle(shuffled_images)\n",
    "    for p,wp in zip(images, shuffled_images):\n",
    "        data.append((os.path.join('files/mapbox.satellite/', p),\n",
    "            os.path.join('files/grayschema/', p), 1, 0))\n",
    "        data.append((os.path.join('files/mapbox.satellite/', p),\n",
    "            os.path.join('files/grayschema/', wp), 0, 1))\n",
    "    random.shuffle(data)\n",
    "        \n",
    "    sat_file_paths = []\n",
    "    schema_file_paths = []\n",
    "    labels = []\n",
    "    for item in data:\n",
    "        sat_file_paths.append(item[0])\n",
    "        schema_file_paths.append(item[1])\n",
    "        labels.append(item[2:4])\n",
    "\n",
    "    return sat_file_paths, schema_file_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_file_paths, schema_file_paths, labels = get_cleaned_data()\n",
    "with open(\"cleaned_data.pickle\", \"wb+\") as f:\n",
    "    pickle.dump((sat_file_paths, schema_file_paths, labels), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_data.pickle\", \"rb\") as f:\n",
    "    sat_file_paths, schema_file_paths, labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splittvt(arr, frac):\n",
    "    count = len(arr)\n",
    "    valid_start = int(count * (1-2*frac))\n",
    "    test_start = int(count * (1-frac))\n",
    "    return arr[:valid_start], arr[valid_start: test_start], arr[test_start:]\n",
    "\n",
    "TST_FRAC = 0.2\n",
    "train_sat_paths, valid_sat_paths, test_sat_paths = splittvt(sat_file_paths, TST_FRAC)\n",
    "train_map_paths, valid_map_paths, test_map_paths = splittvt(schema_file_paths, TST_FRAC)\n",
    "train_labels, valid_labels, test_labels = splittvt(labels, TST_FRAC)\n",
    "print(len(train_sat_paths), len(valid_sat_paths), len(test_sat_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "batch_size = 32\n",
    "DF_DIM = 64\n",
    "\n",
    "def read_img(path, f):\n",
    "    if f == 'png':\n",
    "        return -1 + tf.cast(tf.image.decode_png(tf.read_file(path), channels=1), tf.float32)/128.0\n",
    "    else:\n",
    "        return -1 + tf.cast(tf.image.decode_jpeg(tf.read_file(path)), tf.float32)/128.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, num_outputs, stride, is_training):\n",
    "    d = tf.layers.Conv2D(\n",
    "            num_outputs,\n",
    "            (5, 5),\n",
    "            strides=stride,\n",
    "            padding='SAME'\n",
    "        )(data)\n",
    "    d = tf.layers.BatchNormalization()(d)\n",
    "    d = tf.nn.leaky_relu(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def decode(data, num_outputs, stride, is_training):\n",
    "    d = tf.layers.Conv2DTranspose(\n",
    "            data, num_outputs,\n",
    "            (5, 5),\n",
    "            strides=stride,\n",
    "            padding='SAME'\n",
    "        )(data)\n",
    "    d = tf.layers.BatchNormalization()(d)\n",
    "    d = tf.nn.leacky_relu(d)\n",
    "    return d\n",
    "\n",
    "    \n",
    "def generator(data, is_training):\n",
    "    enc_1 = encode(data, DF_DIM, 2, is_training)      # 128 x 128 x 64\n",
    "    enc_2 = encode(enc_1, DF_DIM * 2, 2, is_training)  # 64 x 64 x 128\n",
    "    enc_3 = encode(enc_2, DF_DIM * 4, 2, is_training)  # 32 x 32 x 256\n",
    "    enc_4 = encode(enc_3, DF_DIM * 8, 2, is_training)  # 16 x 16 x 512\n",
    "    enc_5 = encode(enc_4, DF_DIM * 8, 2, is_training)  # 8 x 8 x 512\n",
    "    enc_6 = encode(enc_5, DF_DIM * 8, 2, is_training)  # 4 x 4 x 512\n",
    "    enc_7 = encode(enc_6, DF_DIM * 8, 2, is_training)  # 2 x 2 x 512\n",
    "    enc_8 = encode(enc_7, DF_DIM * 8, 2, is_training)  # 1 x 1 x 512\n",
    "\n",
    "    dec_7 = decode(enc_8, DF_DIM * 8, 2, is_training)  # 2 x 2 x 512\n",
    "    \n",
    "    dec_6 = decode(tf.concat([dec_7, enc_7], 3), DF_DIM * 8, 2, is_training)  # 4 x 4 x 512\n",
    "    dec_5 = decode(tf.concat([dec_6, enc_6], 3), DF_DIM * 8, 2, is_training)  # 8 x 8 x 512\n",
    "    dec_4 = decode(tf.concat([dec_5, enc_5], 3), DF_DIM * 8, 2, is_training)  # 16 x 16 x 512\n",
    "    dec_3 = decode(tf.concat([dec_4, enc_4], 3), DF_DIM * 4, 2, is_training)  # 32 x 32 x 256\n",
    "    dec_2 = decode(tf.concat([dec_3, enc_3], 3), DF_DIM * 2, 2, is_training)  # 64 x 64 x 128\n",
    "    dec_1 = decode(tf.concat([dec_2, enc_2], 3), DF_DIM , 2, is_training)  # 128 x 128 x 64\n",
    "    result = decode(tf.concat([dec_1, enc_1], 3), 2, is_training)  # 256 x 256 x 3\n",
    "                    \n",
    "    return result\n",
    "\n",
    "def discriminator(data, is_training):\n",
    "    conv = data\n",
    "\n",
    "    conv = encode(conv, DF_DIM, 2, is_training)\n",
    "    conv = encode(conv, DF_DIM * 2, 2, is_training)\n",
    "    conv = encode(conv, DF_DIM * 4, 2, is_training)\n",
    "    conv = encode(conv, DF_DIM * 8, 1, is_training)\n",
    "    \n",
    "    hidden = tf.layers.Flatten()(conv)\n",
    "    logits = tf.layers.Dense(2)(hidden)\n",
    "    return logits\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_is_training = tf.placeholder_with_default(True, shape=())\n",
    "    tf_map_files = tf.placeholder(tf.string, shape=[batch_size])\n",
    "    tf_sat_files = tf.placeholder(tf.string, shape=[batch_size])\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=[batch_size, 2])\n",
    "    \n",
    "    map_imgs = tf.stack([read_img(path, 'png') for path in tf.unstack(tf_map_files)])\n",
    "    print(\"Map imgs, shape\", map_imgs.shape)\n",
    "    sat_imgs = tf.stack([read_img(path, 'jpg') for path in tf.unstack(tf_sat_files)])\n",
    "    print(\"Sat imgs, shape\", sat_imgs.shape)\n",
    "\n",
    "    tf_data = tf.reshape(tf.concat([\n",
    "            map_imgs,\n",
    "            sat_imgs\n",
    "        ],3), [batch_size, IMG_SIZE, IMG_SIZE, 4])\n",
    "    \n",
    "    print(\"Input shape: \", tf_data.shape)\n",
    "    \n",
    "    logits = discriminator(tf_data, tf_is_training)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logits, labels=tf_labels))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    global_step = tf.Variable(0)\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    correct = tf.nn.in_top_k(logits, tf.math.argmax(tf_labels, 1), 1)\n",
    "    count_correct = tf.count_nonzero(correct)\n",
    "    tf.summary.scalar(\"acc\", count_correct/batch_size)\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doEpoch(session, epoch, writer):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    num_batches = (len(train_map_paths) - batch_size) // batch_size + 1\n",
    "    for step in range(num_batches):\n",
    "        offset = (step * batch_size) % (len(train_map_paths) - batch_size + 1)\n",
    "        feed_dict={\n",
    "            tf_map_files: train_map_paths[offset:offset+batch_size],\n",
    "            tf_sat_files: train_sat_paths[offset:offset+batch_size],\n",
    "            tf_labels: train_labels[offset:offset+batch_size]\n",
    "        }\n",
    "        _,l,c,ms, gs = session.run([optimizer, loss, count_correct, merged, global_step], feed_dict=feed_dict)\n",
    "        train_loss += l\n",
    "        train_correct += c\n",
    "        #print(\"TBatch acc:\", 1.0*c/batch_size)\n",
    "\n",
    "        writer.add_summary(ms, gs)\n",
    "\n",
    "    return l, train_loss / num_batches, train_correct\n",
    "    \n",
    "def doValidate(session, writer):\n",
    "    valid_correct = 0\n",
    "    for step in range((len(valid_map_paths) - batch_size) // batch_size + 1):\n",
    "        offset = (step * batch_size) % (len(valid_map_paths) - batch_size + 1)\n",
    "        feed_dict={\n",
    "            tf_map_files: valid_map_paths[offset:offset+batch_size],\n",
    "            tf_sat_files: valid_sat_paths[offset:offset+batch_size],\n",
    "            tf_labels: valid_labels[offset:offset+batch_size],\n",
    "            tf_is_training: False\n",
    "        }\n",
    "\n",
    "        l, c, ms, gs = session.run([loss, count_correct, merged, global_step], feed_dict=feed_dict)\n",
    "        #print(\"Batch acc:\", 1.0*c/batch_size)\n",
    "        valid_correct += c\n",
    "        writer.add_summary(ms, gs)\n",
    "    return (1.0 * valid_correct) / len(valid_map_paths)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "run_id=\"disk_1\"\n",
    "train_writer = tf.summary.FileWriter(\"./summary/nopipeline_train_{}\".format(run_id), graph=graph)\n",
    "valid_writer = tf.summary.FileWriter(\"./summary/nopipeline_valid_{}\".format(run_id))\n",
    "\n",
    "with tf.Session(graph=graph) as session: \n",
    "    tf.global_variables_initializer().run()\n",
    "    print(datetime.now(), \"Initialized.\")\n",
    "    for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "        last_loss, train_loss, correct = doEpoch(session, epoch, train_writer)\n",
    "        accuracy = float('NaN')\n",
    "        accuracy = doValidate(session, valid_writer)\n",
    "        if epoch % 10 == 0:\n",
    "            print(datetime.now(), \"{} loss: {:.6f}, acc: {:.6f} {:.6f}\".format(\n",
    "                epoch, train_loss, accuracy, 1.0*correct/len(train_sat_paths)))\n",
    "        saver.save(session, \"checkpoints/checkpoint_{}\".format(run_id))\n",
    "    print(datetime.now(),\n",
    "          \"Final loss: {:.6f}, acc: {:.6f} {:.6f}\".format(\n",
    "              train_loss, accuracy, 1.0*correct/len(train_sat_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session: \n",
    "    saver.restore(session, \"checkpoints/checkpoint_{}\".format(run_id))\n",
    "\n",
    "    offset = 0\n",
    "    feed_dict={\n",
    "        tf_map_files: train_map_paths[offset:offset+batch_size],\n",
    "        tf_sat_files: train_sat_paths[offset:offset+batch_size],\n",
    "        tf_labels: train_labels[offset:offset+batch_size],\n",
    "        tf_is_training: False\n",
    "    }\n",
    "    l, c, ms, p = session.run([loss, count_correct, merged, prediction], feed_dict=feed_dict)\n",
    "    print(\"Batch acc:\", 1.0*c/batch_size)\n",
    "    print(\"Prediction:\", p)\n",
    "    print(\"GT:\", train_labels[offset:offset+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session: \n",
    "    tf.global_variables_initializer().run()\n",
    "    print \"Initialized.\"\n",
    "\n",
    "    offset = 0\n",
    "    feed_dict={\n",
    "        tf_map_files: train_map_paths[offset:offset+batch_size],\n",
    "        tf_sat_files: train_sat_paths[offset:offset+batch_size],\n",
    "        tf_labels: train_labels[offset:offset+batch_size],\n",
    "        tf_is_training: False\n",
    "    }\n",
    "    p,l,c = session.run([prediction, loss, count_correct], feed_dict=feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
